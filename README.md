# REMOTE-SENSING-SATELLITE-IMAGES-TO-PREDICT-LAND-COVER-CHANGE-IN-SUNDARBANS-USING-TRANSFORMERS

Author: Bhagyasri Nanduri; Supervisors: Dr. Alireza Daneshkhah, Majdi Fanous
Coventry University, Coventry, United Kingdom
 

 
Abstract:
This paper presents a study that uses remote sensing satellite images to predict land cover change in the Sundarbans region by employing a vision transformer model. The study utilizes Landsat 8 satellite images from 2000 to 2021, with the Normalized Difference Vegetation Index (NDVI) used as a measure for tracking vegetation coverage and density. A vision transformer model was developed and trained on the NDVI change data to predict land cover change in the region. The model was evaluated using image classification and image segmentation, with the latter showing promising results. The study highlights the importance of remote sensing and deep learning models in monitoring and predicting land cover changes in regions like the Sundarbans and provides a framework that can be used to monitor land cover changes in other regions, aiding conservation efforts. The developed model can be used as a valuable tool for policymakers and conservationists to identify areas where mangrove habitats are declining and take necessary actions to prevent further degradation.
Keywords: remote sensing, satellite images, land cover change, Sundarbans, vision transformer model, NDVI, vegetation coverage, vegetation density, plant health, deep-learning, image classification, image segmentation, conservation, mangrove habitats, policy-making, ground truth data.
1.	Introduction:
Remote sensing satellite data has become an increasingly important tool in monitoring and predicting land cover changes in different regions of the world. The Sundarbans region, situated at the delta of the Ganges, Brahmaputra, and Meghna rivers, is one such area that has undergone significant land cover changes in recent years. These changes, caused by both natural and human-induced factors, have significant implications for the region's ecological balance and the livelihoods of the local communities. To effectively monitor and predict land cover changes in the Sundarbans, researchers are turning to advanced machine learning techniques such as Transformers, which can handle the large amounts of data generated by remote sensing satellites and provide accurate predictions of land cover changes. This approach can help policymakers and conservationists make informed decisions about the region's future development and preservation.
The Sundarbans is a vast mangrove forest spanning India and Bangladesh and is home to unique flora and fauna. The region faces significant land cover changes caused by natural and human factors. To address these issues, accurate information on land cover changes is necessary. Remote sensing satellite data can be used to monitor and predict land cover changes by collecting data on various parameters. Machine learning techniques like Transformers can efficiently process and analyze the large volume of data generated by satellites. Transformers are suitable for land cover change detection as they can handle large amounts of data and capture complex patterns and relationships in the data. The model breaks down images into smaller patches, which are then processed and analyzed.

2.	Literature Review:
 
The paper by Rahman et al. (2018) investigates the land cover changes in the Sundarbans region between 1995 and 2015 using Landsat data. The study reveals that during this period, the agricultural land increased while the mangrove forest decreased significantly. The authors suggest that this trend poses a threat to the region's ecological balance and could have socio-economic consequences such as coastal erosion and a decrease in fish production. The study's findings highlight the need for sustainable development and informed policymaking to address the expansion of agriculture and the degradation of the mangrove forest in the Sundarbans region. The research provides critical information for policymakers and stakeholders to make informed decisions for the region's sustainable development. Overall, the study provides valuable insights into the impact of land-use changes on the region's ecological balance and calls for immediate action to protect the Sundarbans Forest.
Chen et al. (2021) investigated the potential of using a pre-trained Vision Transformer (ViT) model for land cover change detection using Sentinel-2 satellite imagery. The study showed that the ViT model outperformed traditional convolutional neural networks (CNNs) in terms of accuracy and efficiency, achieving an overall accuracy of 92.35%. The study also highlighted the potential of pre-trained models for image analysis tasks and the ability of Transformers to capture complex patterns and relationships in the imagery. The use of Transformers for land cover change detection has several potential applications, such as monitoring deforestation or urbanization in real-time and tracking changes in agricultural land use or the effects of natural disasters. However, challenges such as the availability and quality of satellite imagery and the need for validation of results with ground-truthing data must be addressed. The study emphasizes the potential of deep learning models for land cover change detection and highlights the need for further research in this area.
Mateo-García et al. (2019) conducted a study on the transferability of deep learning models for cloud detection between two different satellite sensors, Landsat-8 and Proba-V. They used a Convolutional Neural Network (CNN) architecture known as U-Net to train and evaluate cloud detection models on both datasets. The authors demonstrated that the deep learning model could be effectively transferred between the two datasets with some minor modifications, and compared the performance of the deep learning model with other cloud detection methods. The study highlights the potential of deep learning models for cloud detection and the transferability of these models between different satellite sensors. One limitation of the study is that it only examined the transferability of the U-Net model between the Landsat-8 and Proba-V datasets.
3.	Methodology:
3.1 Design Methodology
The recent emergence of Transformer models has shown that they can also be used effectively in image analysis tasks. In this project, the focus is on designing and implementing a Vision Transformer model for land cover change detection using satellite imagery. The project is divided into five sprints, each with a specific focus. 
Sprint 1: Data Pre-processing
During this sprint, I will focus on data pre-processing. I will work on TIF images using resizing and clipping concepts. Also, I will work on converting the input images into patches and embedding them using a learnable embedding matrix.

Sprint 2: NDVI and NDVI change Calculation
During this sprint, I will focus on extracting the bands that are required in calculating NDVI and defining categories to label them. Also, will work on the NDVI change calculation between the adjacent images. 
Sprint 3: Model Design and Implementation
During this sprint, I will focus on designing and implementing the Vision Transformer model using Keras. I will start with a basic architecture and gradually add complexity, experimenting with different hyperparameters and tuning the model to achieve better performance. I will also integrate the model with the pipelines developed in Sprint 1 and 2.
Sprint 4: Model Training and Validation
During this sprint, I will focus on training and validating the model using a training dataset and a validation dataset. I will experiment with different optimization algorithms, learning rates, and batch sizes to achieve the best possible performance. I will also evaluate the model's performance using various metrics, such as accuracy and loss.
Sprint 5: Model Evaluation and Refinement
During this sprint, I will focus on evaluating the model's performance on a test dataset and refining the model based on the results. I will analyze the model's performance using various metrics and identify areas for improvement. I will then refine the model by adjusting the architecture, hyperparameters, or data pre-processing pipeline.
The aim of this project is to demonstrate that Vision Transformer models can be used effectively for land cover change detection using satellite imagery. By focusing on specific tasks in each sprint and gradually building up the model's complexity, the project aims to develop a robust and accurate model that can be used for real-world applications.
3.2 Research Methodology

NDVI Calculation:
“Normalized Difference Vegetation Index (NDVI) quantifies vegetation by measuring the difference between near-infrared (which vegetation strongly reflects) and red light (which vegetation absorbs).”
NDVI = (NIR - Red) / (NIR + Red)
NIR is Near Infra Red light  (Band5)
R is Red light (Band4)

  
Fig: NDVI Calculation (Source: NASA Earth Observatory)

In contrast to other wavelengths, healthy vegetation (chlorophyll) reflects more green and near-infrared (NIR) light. It does, however, absorb more red and blue light. This explains why vegetation seems green to our sight. It would also be effective for vegetation if you could see near infrared. The essential bands with NIR and red are present in satellite sensors like Landsat and Sentinel-2.
This formula produces a value that ranges from -1 to +1. A high NDVI value will result from low reflectance (or low values) in the red channel and high reflectance (or high values) in the NIR channel. Also the opposite. In general, NDVI is a consistent approach to gauge robust vegetation. Vegetation is healthier when the NDVI values are high.
CNN:
Convolutional neural networks (CNNs) use convolutional layers to filter inputs and extract relevant data through a convolution process.
 
Fig: Architecture of CNN

The filters in these layers are adjusted based on learning parameters to select the best features for a given task. CNNs have an input layer, an output layer, and one or more hidden layers with neurons arranged in three dimensions (width, height, and depth). These hidden layers include convolutional layers, pooling layers for reducing dimensionality, normalization layers, and fully connected layers. Non-linear activation functions like ReLU are used in convolutional layers. Max and average pooling are the two types of pooling used to decrease the computational power needed to process data.
LSTM:
LSTM stands for long short-term memory, and it is an RNN architecture that provides short-term memory for thousands of time steps. 
 
The LSTM unit includes a cell, an input gate for adding information to the cell state, an output gate for scaling and regulating the output vector, and a forget gate for eliminating unnecessary or less important information from the cell state. In addition, fully connected layers are often used in CNN architectures for classification purposes. These models can be trained using labelled satellite images and NDVI values, with the dataset split into training, validation, and testing sets for evaluation.
Transformers:

A transformer is a deep learning model that uses the self-attention process and weights the importance of each component of the input data differently.

 
Fig: Transformer architecture

The transformer model follows an encoder-decoder architecture
Encoder: The encoder receives an input sequence, such as a sentence in the source language in the case of machine translation, and produces a fixed-length vector representation of the input, which contains information about the meaning and context of the sentence. This vector is commonly referred to as the "context vector" or "thought vector."
Decoder: The decoder takes the context vector produced by the encoder and uses it to generate a sequence in the target language, such as a translated sentence. The decoder uses an attention mechanism to focus on different parts of the input sequence while generating the output sequence.
Embeddings: Embeddings were the first to use vectors to encode information that could be understood. Therefore, embeddings of the tokens are passed rather than the tokens themselves to the encoder. However, encoding the word position information in the embeddings was a major issue. Standard implanting just planned every token to a n-layered vector however it never encoded the position data. Another embedding known as positional embedding can be used to encode the tokens' positions. Position ids or indices of each sentence are used as inputs in positional embedding rather than the tokens of the words. This empowers the position installing layer to give a helpful portrayal of the positions.
Multi-Head Attention: Transformer employs a more intricate special layer:
It multiplies the extra projection weight matrix for each key, value, and query, and divides the resulting embedding into 8 equal-sized vectors, Also, applies a different 1/8th-dimensional self-attention mechanism to each of them, and then concatenates the outcome.
Masked Multi-Head Attention: Masked Multi-Head Attention is a powerful mechanism that enables the transformer architecture to capture complex dependencies in sequence data while preserving the temporal nature of the data.
In Masked Multi-Head Attention, a mask is applied to the input sequence to ensure that each position can only attend to previous positions in the sequence, preventing information from future positions from leaking into the current position. This is particularly useful in tasks such as language modeling, where the model is trained to predict the next word in a sequence based on the previous words.

Vision Transformer:

The Vision Transformer (ViT) is a powerful deep learning model for computer vision tasks that uses the transformer architecture to capture global and local features in an image. The ViT consists of an input embedding layer, a transformer encoder, and a classification head, which work together to produce high-quality predictions on a wide range of image classification tasks.
 
Fig: Vision Transformer

In the input embedding layer, the image is divided into non-overlapping patches, which are flattened into vectors and embedded using a learnable embedding matrix. These patch embeddings are concatenated with a learnable class token vector, which represents the image as a whole.
The transformer encoder consists of a stack of transformer blocks, each containing a Multi-Head Attention (MHA) layer and a Feed-Forward Network (FFN) layer. The MHA layer is used to capture global information from the image, while the FFN layer applies a non-linear transformation to the output of the MHA layer.
The MHA layer is composed of several sub-layers, including the Query, Key, and Value transformations, and the Attention mechanism. These sub-layers work together to compute a weighted sum of the patch embeddings, where the weights are learned during training and represent the importance of each patch for the task at hand.
The FFN layer applies a non-linear transformation to the output of the MHA layer using two linear layers separated by a non-linear activation function, such as ReLU. The output is then passed through a residual connection and normalized using layer normalization.
The final component, the classification head, maps the output of the final transformer block to the number of output classes, and produces a probability distribution over the output classes using a softmax function. During training, the model is trained to minimize the cross-entropy loss between the predicted and actual output classes. During inference, the model predicts the output class with the highest probability.
Overall, the ViT is a major breakthrough in computer vision and demonstrates the potential of transformer-based models in pushing the boundaries of what is possible in the field of image classification and other related fields.
4.	Experimental Setup:
4.1 Data Collection and Pre-processing
The first stage of the project involves collecting satellite images and calculating NDVI and NDVI change values for each image. I have gathered all the data of landsat8, which are in the format of TIF. 
 
Fig : TIF images
There are 6 scenes for Landsat8, below is the information on the scene and the number of images for each scene. I have mainly concentrated on scenes 136044 and 138044.
Scene	Number of Images
136044	116
136045	180
137044	132
137045	181
138044	176
138045	176
Table1: Scene Details with no.of images
For each image, we have spectral bands. Below are the details for Landsat8.
 
Fig: Spectral band details of Landsat 8 
(Source: usgs.gov)
From the above, Band 4 is Red, and Band 5 is NIR(Near Infra Red)
  
Fig: Actual Image             Fig: Clipped Image
from scene 136044              from scene 136044

  
Fig: Actual Image             Fig: Clipped Image
from scene 138044              from scene 138044

4.2 NDVI:
I have calculated the NDVI (Normal Difference Vegetation Index) using the bands  B4 and B5. In Landsat8, Band 4 represents RED and Band 5 represents Near Infra RED (NIR). the formula to calculate NDVI is as follows. 
NDVI = ( band 5 – band 4 )/( band 5 + band 4 )

The images after calculating the NDVI and applying them are as follows.
 
Fig: NDVI calculated images from scene 136044

 
Fig: NDVI calculated images from scene 138044

4.3 NDVI Change:
NDVI change is the difference in NDVI values between two images taken in different time periods. Identifying the vegetation in a particular region can be done using the NDVI change. This can be done in many ways, 
1. Subtracting NDVI values of the images 
2. By calculating the percentage change in NDVI 
Where the positive value indicates an increase in vegetation and the negative value gives a decrease in vegetation.
  
Fig: Understanding NDVI change calculation from scene 136044

  
Fig: Understanding NDVI change calculation from scene 138044

The images after calculating the NDVI change are as follows.

 
Fig: NDVI Change calculated images from scene 136044

 
	Fig: NDVI Change calculated images from scene 138044	

This NDVI change analysis helps us understand the landcover changes and thus the vegetation changes in the Sundarbans region

The labels for NDVI change are defined below:

For values from -1 to -0.5 it is a High Decrease (1), from -0.5 to 0it is a Decrease (2) if the value is 0 then No Change (3), from 0 to 0.5 it is considered to be an Increase (4) from 0.5 to 1 it is High Increase (5).

4.4 Model Development using CNN and LSTM
The second stage of the project will involve developing CNN and LSTM models to classify vegetation types based on the pre-processed satellite images and calculated NDVI and NDVI change values.

The model accepts input sequences with dimensions of (1, 100, 100) and runs them through a 400-unit LSTM layer designed to capture the temporal dependencies in the input sequences. The batch size is set to 1 and the stateful argument is set to True, indicating that the network's internal state is carried over between batches. The LSTM layer's output is routed through a sequence of 1D convolutional layers, each with a unique number of filters, kernel size, and activation function. These layers are intended to extract characteristics from the input sequences.

The output of the convolutional layers is then passed via a Flatten layer, which transforms the 3D output to a 2D output. After that, the output is transmitted via a RepeatVector layer, which repeats the output sequence 100 times. This is done to create the output sequence for the second set of LSTM layers, which will take as input the repeated output sequence.

The RepeatVector layer's output is routed through two further LSTM layers of 200 and 100 units, respectively. The final LSTM layer's output is then routed via a TimeDistributed layer with a Dense layer, resulting in a sequence of 100-dimensional output vectors. The Adam optimizer and the mean squared error loss function are then used to construct the model. The mean squared error, mean absolute error, and accuracy measures are used to evaluate the model.
The CNN model is used to extract spatial features from the satellite images, while the LSTM model will be used to capture temporal features from the NDVI and NDVI change values. The output of the CNN and LSTM models will be combined using a fully connected layer to generate the final classification output.

4.5 Development of a Vision Transformer using Keras
The next stage of the project will involve developing a vision transformer using Keras. The vision transformer will be based on the Transformer architecture. 

The images from the NDVI change output are the input images to our Vision transformer. The masked forms of these images are given as labels.

Experiment 1: Image classification using Vision Transformer
INPUT: The input image we have given is of size (1, 288,432,3). The NDVI change images from two scenes 136044,138044 are considered to be the input data for our model. 
Scene	No.of images 
136044	28
138044	43
Total 	71

Hence the dataset shape is (71, 288, 432, 3)
Initially, the necessary libraries are imported. Path to the image data set has been set and resized the image to be 100x100 format. Now the new shape of the image data set is (71, 100, 100, 3).
And the masked images of the two scenes are given as labels to our model. The labels are in the shape (71, 288, 432, 3) initially. As they are grayscale images, we have converted them to be (71, 288, 432, ) and as we are dealing with a single label for each image, we have suppressed it to give a shape of (71, 1, 1) 
SMOTE: SMOTE is a machine learning algorithm used to address class imbalance problems. It creates synthetic samples of the minority class by interpolating new instances between existing ones. It selects an instance from the minority class, finds its k-nearest neighbors, and creates synthetic instances by interpolating between the selected instance and its k-nearest neighbors. This process continues until the desired number of synthetic instances is reached. SMOTE-generated synthetic instances are similar to the existing minority class instances but not exact copies, which can help to reduce overfitting and improve the generalization performance of the machine learning model.
TRAIN-TEST-SPLIT: The train_test_split function splits the image data and labels data into training and testing datasets. Here, the test size is set to 0.2 which defines the testing data set would be allocated 20% of the main datasets and training would be given 80%. The random_state is set to 42 which ensures the same data split occurs each time the execution is done. And, Stratify is set to label data which does the equal proportions of each label are given to training and testing datasets.
DEFINING HYPER-PARAMETERS: Hyper-parameters for training transformer-based neural networks include learning rate, weight decay, batch size, number of epochs, image size, patch size, number of patches, projection dimension, number of attention heads, transformer units, transformer layers, and MLP head units. These parameters impact the model's performance and training time.
DATA AUGMENTATION: Data Augmentation is used to increase the diversity and size of the dataset during training. It includes transformations such as normalization, resizing, random horizontal flipping, random rotation, and random zoom. The normalization layer normalizes the pixel values of input images, the resizing layer resizes the input images, and the random flip, rotation, and zoom layers randomly apply horizontal flipping, rotation, and zoom to the input images, respectively, to increase the variety in the training data and make the model more robust.
MLP [Multi-layer perception]: The MLP is a neural network consisting of fully connected layers with ReLU activation and dropout layers to prevent overfitting. It takes an input tensor, a list of integers specifying the number of units in each hidden layer, and a dropout rate. The function adds fully connected and dropout layers based on the provided arguments and returns the output tensor. The activation function used is "tf.nn.gelu," which is known to improve performance compared to ReLU in some cases.
PATCHES: This layer extracts non-overlapping patches from input images to process large images or apply convolutional neural networks to image patches. It takes an argument called patch_size, which is an integer specifying the size of the square patches to extract from the images. The call() method uses TensorFlow's image module function called extract_patches() to extract patches from the input images. It takes five arguments: images, sizes, strides, rates, and padding. The output of extract_patches() is a tensor of shape [batch_size, num_patches_h, num_patches_w, patch_size * patch_size * num_channels]. Then the output is reshaped to [batch_size, num_patches, patch_size * patch_size * num_channels] using the tf.reshape() function. Finally, the call() method returns the output tensor, which can be used as input to other layers in a Keras model.
PATCH ENCODER: The Patch encoder class is used to encode patches of an image using projection and positional embeddings in a transformer-based image classification model. It extends TensorFlow's "layers.Layer" class and has two arguments: num_patches and projection_dim, which determine the number of patches used and the dimensions of the encoded patches. The class has two attributes: a dense layer that maps input patches to a projection dimension vector and an embedding layer that maps the position of each patch to a projection dimension vector. The "call" method generates positions ranging from 0 to num_patches, adds positional embeddings, applies the dense layer to the patch, and returns the encoded patch.
VIT CLASSIFIER: The function "create_vit_classifier" uses the Keras API to implement a Vision Transformer architecture for image classification. It takes input data and applies data augmentation to it using the "data_augmentation" method. Patches are created from the augmented data using the "Patches" method, which are then encoded using the "PatchEncoder" method. Multiple Transformer blocks are created using a for loop. Each block contains a layer normalization step, a multi-head attention layer, another layer normalization step, and a multi-layer perceptron (MLP) layer. Dropout layer is applied to prevent overfitting. The output is flattened and passed through an MLP layer to generate logits for each class. Finally, a Keras model is created using the input layer and the output logits, which is returned by the function.
RUN EXPERIMENT: It compiles and fits the given model on the training data, using the specified hyperparameters. It uses the AdamW optimizer and binary cross-entropy loss with accuracy as the evaluation metric. During training, it saves the weights of the best-performing model on the validation set to a temporary checkpoint file. It returns a history object containing the training and validation loss and accuracy values over the epochs.
Experiment 2: Image segmentation using Vision Transformer
The implementation of image segmentation is exactly the same as image classification. Here, we are focusing to convert the image into pixels and give it to the model. 
Single image to pixels without reducing the size:
The input images are of shape 288 x 432.  We have 28 images in the 136044 scene. If we convert them into pixels, it would be 3483648 pixels. Processing the huge pixels has a risk of high computational power. It is the same with labels. 
Instead, I have choosed one image and converted it into pixels the initial shape of the image is (1, 288, 432, 3) and after the conversion, the shape of the image is (124416, 1, 1, 3) and the shape of labels for this image is (124416, 1). 
After splitting it into train and split, this is given to a vision transformer. 
Four images to pixels with a size of 100 x 100:
I have reduced the size of the images to 100 x 100. I have now chosen four images. 
Converting these four images to pixels, the shape of (4, 100, 100, 3) has been changed to (40000, 1, 1, 3). And the labels have been changed from (4, 100, 100, 1) to (40000, 1). This is given to the vision transformer and proceeded with the same steps as followed in Experiment 1.
Scene 136044 images to pixels with a size of 32 x 32:
Considering Scene 136044, 
We have 28 images in 288 x 432 format. Our Initial shape of the image dataset is (28, 288, 432, 3) as they are RGB images and the label dataset is (28, 288, 432, 1) as they are grayscale images. 
Converting into pixels, the image dataset would become (3483648, 3) the and label dataset would be (3483648, 1)
As it is very time complex to process the above huge number of pixels, I have resized them to be in 32 x 32 format. Hence the shape of the image dataset is (28, 32, 32, 3), in pixels it is (28672, 1, 1, 3)  and the label dataset shape is (28, 32, 32, 1), in pixels it is (28,672, 1).  This is given to the vision transformer and proceeded with the same steps as in Experiment 1.
5.	Results:
5.1 Evaluation Metrics: 
Accuracy, Precision, Recall, F1 Score, Confusion Matrix, and ROC Curve are the commonly used evaluation metrics for machine learning classification models. Accuracy measures how frequently the classifier predicts correctly, while precision measures how many of the predicted positive cases were actually true positive cases. Recall measures how many actual positive cases were properly anticipated by the model. F1 Score is the harmonic mean of precision and recall, and penalizes extreme values more severely. The Confusion Matrix is a table that shows the combinations of expected and actual values and is often used to evaluate the performance of classification models. ROC Curve is the plot between sensitivity and (1- specificity) and is used to evaluate the performance of binary classification models.
The research involved multiple experiments, including image classification and image segmentation using a Vision Transformer. For image segmentation, experiments were conducted using a single image to pixels without reducing the size, four images to pixels with a size of 100 x 100, and scene 136044 images to pixels with a size of 32 x 32.
5.2 Model Results: 
IMAGE CLASSIFICATION USING VIT:
In the image classification using a vision transformer, The NDVI change images are the input Images and the masked images are the labels. 
 
Fig: NDVI Change Images(Input Images)
  
Fig: Masked Images(Labels))

LOSS & ACCURACY CURVES: 
The Loss curve of the image classification for scenes 136044 & 138044 combinely with 71 images is as below.
  
Fig: Loss curve of Image classification
The accuracy for this is 75% and the curve for image classification is below. 
 
Fig: Accuracy curve of Image classification




CLASSIFICATION REPORT:
 
Fig: Classification report of Image Classification

CONFUSION MATRIX:
 
Fig: Confusion Matrix

IMAGE SEGMENTATION USING VIT:
Single image to pixels without reducing the size:
Here, We have converted our image into pxiels. The Image size is 288 x 432. It is an RGB image. Hence our input image shape is (1, 288, 432, 3). While converting the image into pixels, the image shape has become (124416, 1, 1, 3). Now, the pixel level segmentation is performed by the vision transformer. 
  

LOSS AND ACCURACY CURVES:
         
Fig: Loss curve                                                                       Fig: Accuracy curve

CLASSIFICATION REPORT:
 
Fig: Classification report
CONFUSION MATRIX:
 
Fig: Confusion Matrix

Four images to pixels with a size of 100 x 100
INPUT IMAGES 
 

LABELS
 
LOSS CURVE
 
Fig: Loss curve
CLASSIFICATION REPORT
 
Fig: Classification report
CONFUSION MATRIX
 
Fig: Confusion Matrix


Scene 136044 images to pixels with a size of 32 x 32:
       
LOSS AND ACCURACY CURVES:
      
                            Fig: Loss curve                                                        Fig: Accuracy curve

CLASSIFICATION REPORT
 
Fig: Classification report






CONFUSION MATRIX:
 
Fig: Confusion Matrix

5.3 MODEL COMPARISON

Metrics	Models Implemented
	CNN	VISION TRANSFORMER
	Scene 136044	Scene 138044	Image Classification	Image Segmentation
				Single Image	Four Images	Scene 136044
Accuracy	0.895	0.69	0.75	0.98	0.85	0.86
Precision	0.89	0.71	0.76	0.98	0.9	0.86
Recall	0.895	0.79	0.75	0.98	0.86	0.86
F1 Score	0.85	0.66	0.74	0.98	0.85	0.86
Table: Model Comparision
6.Social, Legal, Ethical And Professional Considerations:
In this research project, it was ensured that social, ethical, legal, and professional considerations were taken into account. The satellite image dataset used was a publicly available secondary dataset, and no personal or identifiable data was included. While satellite imagery can raise concerns about privacy, the images used in this project had a low resolution and did not contain personal information.
It is important to consider the broader implications of image data and how it could potentially be used in the future. While the image segmentation techniques developed in this project have the potential to be used for surveillance or other purposes that could raise ethical concerns, it is crucial to conduct research in a responsible and ethical manner.
An ethical assessment was conducted, and the project was categorized as low-risk, with an ethics approval certificate attached to the appendix section. It was ensured that no social, ethical, legal, or professional policies were breached during the project.
7. Discussion and Conclusions:
The discussion chapter of this research can be divided into two main sections: the first section focuses on image classification, while the second section discusses pixel-level segmentation. In both sections, the key findings, limitations, and potential directions for future research are presented.
Image Classification:
The results of this study indicate that Vision Transformers can be a powerful tool for image classification, outperforming traditional CNNs in certain scenarios. However, the accuracy achieved in this study was only 75%, which suggests that there is room for improvement. One potential explanation for the relatively low accuracy is data loss during the compression process. To improve accuracy, future research could explore techniques for reducing data loss, such as data augmentation or more advanced compression methods.
Another potential approach for improving accuracy could be to increase the size of the input images. While the 100x100 image size used in this study allowed for efficient computation, it may not provide enough resolution for accurate classification. Future research could explore larger image sizes and evaluate their impact on accuracy.
Pixel-Level Segmentation:
The findings of this study indicate that pixel-level segmentation can be an effective tool for accurately labeling each pixel in an image. The accuracy achieved in this study was 98%, which is quite promising. However, processing all of the pixels in an image can be computationally intensive, with each epoch taking over 30 minutes.
To address this issue, future research could explore techniques for improving computational efficiency. For example, using more advanced hardware or exploring parallel processing methods could reduce the time required for each epoch. Another approach could be to use techniques such as transfer learning to train a model on a smaller dataset and then fine-tune it on a larger dataset.
The study also found that the accuracy of pixel-level segmentation is affected by the size of the images. As the image size is reduced, the accuracy decreases. However, it is also important to consider the impact of data loss on accuracy. In this study, the accuracy decreased when labels were compressed, indicating that data loss could be a factor in the reduced accuracy. To improve accuracy, future research could explore techniques for reducing data loss, such as data augmentation or more advanced compression methods.
Conclusion:
In conclusion, this study has demonstrated the potential of Vision Transformers for image classification and pixel-level segmentation. However, there is still much work to be done to improve the accuracy of these models. Future research could explore techniques for reducing data loss, increasing the size of the input images, improving computational efficiency, and exploring transfer learning. By addressing these issues, we can develop more reliable and accurate models for image classification and pixel-level segmentation, which have important applications in a variety of real-world scenarios.
References:
1.	Rahman, M. M., Islam, M. M., Haque, M. A., & Hossain, M. S. (2018). Land cover changes in the Sundarbans region using Landsat data between 1995 and 2015. Regional Science Inquiry, 10(2), 1-12.
2.	Chen, X., Du, B., Sun, Z., Liu, S., Liu, S., & Sun, G. (2021). Transformer-Based Land Cover Change Detection Using Sentinel-2 Imagery. Remote Sensing, 13(12), 2308. https://doi.org/10.3390/rs13122308
3.	Mateo-García, G., Fernández-Sarría, A., Camps-Valls, G., & Mateo, J. (2019). Cloud detection transferability between Landsat-8 and Proba-V using deep learning. Remote Sensing, 11(10), 1165. https://doi.org/10.3390/rs11101165
4.	The Sundarbans https://whc.unesco.org/en/list/798/#:~:text=The%20Sundarbans%20mangrove%20forest%2C%20one,Heritage%20site%20inscribed%20in%201987.
5.	https://zerocarbon-analytics.org/archives/justice/loss-and-damage-in-the-sundarbans#:~:text=Land%20mass%20is%20declining%20year%20by%20year&text=Satellite%20imagery%20shows%20the%20sea,shoreline%20in%20the%20last%20forty.

